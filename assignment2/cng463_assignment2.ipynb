{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3Qjh8mRHqD"
      },
      "source": [
        "# Assignment 2: N-grams and Language Identification\n",
        "## CNG463 - Introduction to Natural Language Processing\n",
        "### METU NCC Computer Engineering | Fall 2025-26\n",
        "\n",
        "**Student Name: Nagme Cagla Golcu**  \n",
        "**Student ID: 2526366**  \n",
        "**Due Date:** 16 November 2025 (Sunday) before midnight\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDEnwcxRHqE"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This assignment focuses on:\n",
        "1. Building **character-based** 2-gram and 3-gram language models with Laplace smoothing\n",
        "2. Sentence-based language identification using 10-fold cross-validation\n",
        "3. Evaluation using accuracy, precision, recall, and F1-score\n",
        "4. Comparison and analysis\n",
        "\n",
        "**Note:** For language identification, we use **character n-grams** rather than word n-grams because they better capture language-specific patterns like letter combinations, diacritics, and writing systems.\n",
        "\n",
        "**Grading:**\n",
        "- Written Questions (7 × 4 pts): **28 pts**\n",
        "- Code Tasks with TODO (11 total): **72 pts** distributed by effort level:\n",
        "  - Simple tasks: 4 pts each (2 cells)\n",
        "  - Moderate tasks: 6 pts each (4 cells)\n",
        "  - Complex tasks: 8 pts each (5 cells)\n",
        "- **Total: 100 pts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srF42PZXRHqP"
      },
      "source": [
        "---\n",
        "\n",
        "## Pre-Submission Checklist\n",
        "\n",
        "- [ ] Name and student ID at top\n",
        "- [ ] No cells are added or removed\n",
        "- [ ] All TODO sections completed\n",
        "- [ ] All questions answered\n",
        "- [ ] Code runs without errors\n",
        "- [ ] Results tables included\n",
        "- [ ] Run All before saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0Xw3YQRHqE"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwyLNfKaRHqE"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Tuple, Dict\n",
        "import re\n",
        "\n",
        "# Scikit-learn for cross-validation and metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr-UNnAGRHqF"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: Corpus Preparation and Statistics (22 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5k13c4MRHqF"
      },
      "source": [
        "## 1.1: Upload Corpus Files\n",
        "\n",
        "Prepare your text files in **two different languages** (accepted formats: `.txt`, `.pdf`, or `.docx`). When you run the cell below, you'll be prompted to upload files for each language separately. Make sure your files contain substantial text (reports, essays, or similar content from other courses). Each language requires at least **5000** words in its corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cb6tjNLmRHqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "ad28ac02-a30b-478a-cf90-1aa95f2884f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your ENGLISH corpus file(s):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5bef5fbd-1b60-4064-8c1b-751ef6d91723\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5bef5fbd-1b60-4064-8c1b-751ef6d91723\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pg46.txt to pg46.txt\n",
            "\n",
            "Upload your SECOND LANGUAGE corpus file(s):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f9b1d10e-3c49-43fa-9f70-10c77c54bd8b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f9b1d10e-3c49-43fa-9f70-10c77c54bd8b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving turkish.txt to turkish.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your ENGLISH corpus file(s):\")\n",
        "english_files = files.upload()\n",
        "\n",
        "print(\"\\nUpload your SECOND LANGUAGE corpus file(s):\")\n",
        "second_lang_files = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDDP9gORHqF"
      },
      "source": [
        "## 1.2: Load and Preprocess Data (12 points)\n",
        "\n",
        "Load your uploaded files, extract text, preprocess, split into sentences, and tokenize. You'll need helper functions to handle different file formats.\n",
        "\n",
        "**Steps:**\n",
        "1. Read files based on format (`.txt`, `.pdf`, `.docx`) and combine them into single text for each language\n",
        "2. Apply preprocessing (e.g., lowercasing, handling punctuation)\n",
        "3. Split each corpus into individual sentences\n",
        "4. Tokenize each sentence into words (for statistics)\n",
        "5. Store the results as two lists of tokenized sentences\n",
        "\n",
        "**Important:** You'll use word tokenization for calculating statistics, but for the n-gram models in Task 2, you'll work with character n-grams directly on the sentence strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FI_9lAXTRHqF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "def read_txt_file(filename: str) -> str:\n",
        "    \"\"\"Read a .txt file and return its content.\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def read_pdf_file(filename: str) -> str:\n",
        "    \"\"\"Read a .pdf file and return its text content.\"\"\"\n",
        "    # Şu an pdf kullanmıyoruz, o yüzden boş dönebilir.\n",
        "    return \"\"\n",
        "\n",
        "def read_docx_file(filename: str) -> str:\n",
        "    \"\"\"Read a .docx file and return its text content.\"\"\"\n",
        "    # Şu an docx kullanmıyoruz, o yüzden boş dönebilir.\n",
        "    return \"\"\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    \"\"\"Split text into sentences.\"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "def tokenize_sentence(sentence: str) -> List[str]:\n",
        "    \"\"\"Tokenize a sentence into words.\"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    tokens = sentence.split()\n",
        "    return tokens\n",
        "\n",
        "# 1. Read and combine files for each language\n",
        "\n",
        "lang1_text = \"\"\n",
        "for filename in english_files.keys():\n",
        "    if filename.endswith(\".txt\"):\n",
        "        lang1_text += read_txt_file(filename) + \" \"\n",
        "    elif filename.endswith(\".pdf\"):\n",
        "        lang1_text += read_pdf_file(filename) + \" \"\n",
        "    elif filename.endswith(\".docx\"):\n",
        "        lang1_text += read_docx_file(filename) + \" \"\n",
        "\n",
        "lang2_text = \"\"\n",
        "for filename in second_lang_files.keys():\n",
        "    if filename.endswith(\".txt\"):\n",
        "        lang2_text += read_txt_file(filename) + \" \"\n",
        "    elif filename.endswith(\".pdf\"):\n",
        "        lang2_text += read_pdf_file(filename) + \" \"\n",
        "    elif filename.endswith(\".docx\"):\n",
        "        lang2_text += read_docx_file(filename) + \" \"\n",
        "\n",
        "# 2. Apply preprocessing (lowercasing, removing extra whitespace)\n",
        "def preprocess(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)   # birden fazla boşluk → tek boşluk\n",
        "    return text.strip()\n",
        "\n",
        "lang1_text = preprocess(lang1_text)\n",
        "lang2_text = preprocess(lang2_text)\n",
        "\n",
        "# 3. Split each corpus into sentences\n",
        "lang1_sentences = split_into_sentences(lang1_text)\n",
        "lang2_sentences = split_into_sentences(lang2_text)\n",
        "\n",
        "# 4. Tokenize each sentence\n",
        "lang1_sentences_tokenized = [tokenize_sentence(s) for s in lang1_sentences]\n",
        "lang2_sentences_tokenized = [tokenize_sentence(s) for s in lang2_sentences]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO:\n",
        "#\n",
        "# 1. Read and combine files for each language\n",
        "#    - Loop through lang1_files and lang2_files\n",
        "#    - Use appropriate read function based on file extension\n",
        "#    - Combine all text into lang1_text and lang2_text\n",
        "#\n",
        "# 2. Apply preprocessing to both lang1_text and lang2_text\n",
        "#    (e.g., lowercasing, removing extra whitespace)\n",
        "#\n",
        "# 3. Split each corpus into sentences using split_into_sentences()\n",
        "#\n",
        "# 4. Tokenize each sentence using tokenize_sentence()\n",
        "#\n",
        "# Note: These tokenized sentences will be used for statistics in Task 1.\n",
        "# In Task 2, you'll work with the raw sentence strings for character n-grams.\n",
        "\n",
        "# At the end, you should have:\n",
        "# lang1_sentences = [sent1, sent2, ...]\n",
        "# lang2_sentences = [sent1, sent2, ...]\n",
        "# lang1_sentences_tokenized = [[word1, word2, ...], [word1, word2, ...], ...]\n",
        "# lang2_sentences_tokenized = [[word1, word2, ...], [word1, word2, ...], ...]\n",
        "\n",
        "\n",
        "# [8 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg18ezdTRHqG"
      },
      "source": [
        "**Question 1.1:** What preprocessing choices did you make and why? (3-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLjQTbejRHqG"
      },
      "source": [
        "**In order for the language model to treat words with different capitalizations as a single unit, I first preprocessed all of the text by changing it to lowercase.  In order to guarantee cleaner sentence splitting and prevent the creation of empty or meaningless tokens, I then eliminated extra whitespace.  A straightforward punctuation-based regex that offers consistent segmentation for both languages was used to identify sentence boundaries.  Whitespace-based splitting was then used to tokenize each sentence, resulting in distinct word-level units for statistical analysis.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prIqhFYyRHqG"
      },
      "source": [
        "## 1.3: Basic Statistics (10 points)\n",
        "\n",
        "Calculate and display key statistics for both language corpora to understand their characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yvSoUoATRHqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc60d995-2750-433c-f244-1c345a77028e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- STATISTICS --------- Language1 |Language2\n",
            "Total characters:              175116  |  45351\n",
            "Special characters:            7993    |  1215\n",
            "Character vocabulary size:     64      |  48\n",
            "Total words count:             31611   |  5130\n",
            "Word vocabulary size:          7235    |  145\n",
            "Sentence count:                1629    |  315\n",
            "Avg sentence length(in words):  19.41  |  16.29\n"
          ]
        }
      ],
      "source": [
        "# --- STATISTICS FOR LANGUAGE 1 ---\n",
        "\n",
        "# Total character count\n",
        "lang1_total_characters = sum(len(sentence) for sentence in lang1_sentences)\n",
        "\n",
        "# Special character / punctuation count\n",
        "lang1_special_characters = sum(\n",
        "    sum(1 for ch in sentence if not ch.isalnum() and not ch.isspace())\n",
        "    for sentence in lang1_sentences\n",
        ")\n",
        "\n",
        "# Unique character vocabulary size\n",
        "lang1_char_vocabulary = len(set(\"\".join(lang1_text)))\n",
        "\n",
        "# Total word count\n",
        "lang1_total_words = sum(len(tokens) for tokens in lang1_sentences_tokenized)\n",
        "\n",
        "# Unique word vocabulary size\n",
        "lang1_word_vocabulary = len(set(word for sent in lang1_sentences_tokenized for word in sent))\n",
        "\n",
        "# Sentence count\n",
        "lang1_sentence_count = len(lang1_sentences)\n",
        "\n",
        "# Average sentence length (in words)\n",
        "lang1_avg_sentence_length = lang1_total_words / lang1_sentence_count if lang1_sentence_count > 0 else 0\n",
        "\n",
        "\n",
        "# --- STATISTICS FOR LANGUAGE 2 ---\n",
        "\n",
        "lang2_total_characters = sum(len(sentence) for sentence in lang2_sentences)\n",
        "\n",
        "lang2_special_characters = sum(\n",
        "    sum(1 for ch in sentence if not ch.isalnum() and not ch.isspace())\n",
        "    for sentence in lang2_sentences\n",
        ")\n",
        "\n",
        "lang2_char_vocabulary = len(set(\"\".join(lang2_text)))\n",
        "\n",
        "lang2_total_words = sum(len(tokens) for tokens in lang2_sentences_tokenized)\n",
        "\n",
        "lang2_word_vocabulary = len(set(word for sent in lang2_sentences_tokenized for word in sent))\n",
        "\n",
        "lang2_sentence_count = len(lang2_sentences)\n",
        "\n",
        "lang2_avg_sentence_length = lang2_total_words / lang2_sentence_count if lang2_sentence_count > 0 else 0\n",
        "\n",
        "\n",
        "# --- PRINT RESULTS SIDE BY SIDE ---\n",
        "\n",
        "print(\"------- STATISTICS --------- Language1 |Language2\")\n",
        "\n",
        "print(\"Total characters:             \", lang1_total_characters, \" | \", lang2_total_characters)\n",
        "print(\"Special characters:           \", lang1_special_characters, \"   | \", lang2_special_characters)\n",
        "print(\"Character vocabulary size:    \", lang1_char_vocabulary, \"     | \", lang2_char_vocabulary)\n",
        "print(\"Total words count:            \", lang1_total_words, \"  | \", lang2_total_words)\n",
        "print(\"Word vocabulary size:         \", lang1_word_vocabulary, \"   | \", lang2_word_vocabulary)\n",
        "print(\"Sentence count:               \", lang1_sentence_count, \"   | \", lang2_sentence_count)\n",
        "print(\"Avg sentence length(in words): \", round(lang1_avg_sentence_length,2), \" | \", round(lang2_avg_sentence_length,2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-PSkjPGRHqG"
      },
      "source": [
        "**Question 1.2:** What are the key differences between your two corpora? (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS9Y_nWwRHqG"
      },
      "source": [
        "**There are a lot more characters, words, and a much richer vocabulary in the English text corpus than in the Turkish texts.  In addition, their average sentence lengths are longer and they have a lot more sentences, which indicates that their sentence structures are more intricate or descriptive.  The Turkish texts, on the other hand, are substantially shorter and contain a smaller vocabulary, which is indicative of shorter texts or more repetitive linguistic patterns.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMnhFykGRHqG"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Character N-gram Language Identification (58 points)\n",
        "\n",
        "**Baseline (46 pts):** Implement character-based 2-gram and 3-gram models, run 10-fold CV, report accuracy.  \n",
        "**Creativity (12 pts):** Out-of-vocabulary analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugQ7EB70RHqG"
      },
      "source": [
        "## 2.1: Implement Character N-gram Models (12 points)\n",
        "\n",
        "Implement the `CharNgramLanguageModel` class with Laplace smoothing using NLTK's n-gram utilities. The model should count **character** n-grams during training and calculate sentence probabilities with smoothing.\n",
        "\n",
        "**Key difference from word n-grams:** Instead of tokenizing sentences into words, you'll work with individual characters in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tKiKoqstRHqG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams, pad_sequence\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from typing import List\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "class CharNgramLanguageModel:\n",
        "    \"\"\"\n",
        "    Character-based N-gram language model with Laplace (add-1) smoothing using NLTK.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int = 2):\n",
        "        \"\"\"\n",
        "        Initialize the character n-gram model.\n",
        "\n",
        "        Args:\n",
        "            n: Order of n-gram (2 for bigram, 3 for trigram)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.model = Laplace(n)\n",
        "\n",
        "    def train(self, sentences: List[str]):\n",
        "        \"\"\"\n",
        "        Train the model on a list of sentences.\n",
        "\n",
        "        Args:\n",
        "            sentences: List of sentences (each sentence is a string)\n",
        "        \"\"\"\n",
        "        # Convert each sentence to list of characters\n",
        "        char_sequences = [list(s) for s in sentences]\n",
        "\n",
        "        # Prepare n-gram training data with padding\n",
        "        train_ngrams, vocab = padded_everygram_pipeline(self.n, char_sequences)\n",
        "\n",
        "        # Fit Laplace n-gram model\n",
        "        self.model.fit(train_ngrams, vocab)\n",
        "\n",
        "    def get_probability(self, sentence: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the probability of a sentence.\n",
        "        \"\"\"\n",
        "        import math\n",
        "\n",
        "        # Convert sentence to list of characters\n",
        "        chars = list(sentence)\n",
        "\n",
        "        # Pad the character sequence\n",
        "        padded_chars = pad_sequence(\n",
        "            chars,\n",
        "            n=self.n,\n",
        "            pad_left=True,\n",
        "            pad_right=True,\n",
        "            left_pad_symbol=\"<s>\",\n",
        "            right_pad_symbol=\"</s>\",\n",
        "        )\n",
        "\n",
        "        # Generate n-grams\n",
        "        sent_ngrams = ngrams(padded_chars, self.n)\n",
        "\n",
        "        # Compute log probability\n",
        "        log_prob = 0.0\n",
        "        for ng in sent_ngrams:\n",
        "            context = ng[:-1]\n",
        "            char = ng[-1]\n",
        "            prob = self.model.score(char, context)\n",
        "\n",
        "            if prob <= 0:\n",
        "                prob = 1e-12\n",
        "\n",
        "            log_prob += math.log(prob)\n",
        "\n",
        "        return math.exp(log_prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwxpPMNwRHqG"
      },
      "source": [
        "### Spot Check: Inspect Your N-gram Models\n",
        "\n",
        "After implementing the model, train sample models on both languages and inspect what they learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TT9Q6WH8RHqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825ac302-cb94-410f-b152-421a8f646142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Vocabulary sizes =====\n",
            "Lang1 2-gram vocab size: 67\n",
            "Lang1 3-gram vocab size: 67\n",
            "Lang2 2-gram vocab size: 51\n",
            "Lang2 3-gram vocab size: 51\n",
            "\n",
            "===== Sample 2-gram char n-grams (Lang1) =====\n",
            "'<s>\\ufeff' -> count=1\n",
            "'<s>y' -> count=30\n",
            "'<s>i' -> count=195\n",
            "'<s>t' -> count=237\n",
            "'<s>m' -> count=32\n",
            "'<s>d' -> count=23\n",
            "'<s>c' -> count=42\n",
            "'<s>s' -> count=109\n",
            "'<s>o' -> count=27\n",
            "'<s>b' -> count=69\n",
            "\n",
            "===== Sample 2-gram char n-grams (Lang2) =====\n",
            "'<s>b' -> count=45\n",
            "'<s>f' -> count=45\n",
            "'<s>n' -> count=45\n",
            "'<s>k' -> count=90\n",
            "'<s>g' -> count=45\n",
            "'<s>a' -> count=45\n",
            "'bi' -> count=360\n",
            "'il' -> count=360\n",
            "'im' -> count=360\n",
            "'in' -> count=1125\n",
            "\n",
            "Example Lang1 sentence: ﻿the project gutenberg ebook of a christmas carol in prose; being a ghost story  ...\n",
            "P_2gram(Lang1) = 4.0905016374237176e-258\n",
            "P_3gram(Lang1) = 2.868153326444669e-206\n",
            "\n",
            "Example Lang2 sentence: bilim (1), evrenin işleyişini anlamak için sistematik gözlem, deney ve mantıksal ...\n",
            "P_2gram(Lang2) = 5.1918384750125995e-114\n",
            "P_3gram(Lang2) = 1.0824938417945864e-72\n"
          ]
        }
      ],
      "source": [
        "# TODO: Train sample models and inspect them\n",
        "#\n",
        "# 1. Create 2-gram and 3-gram models for both languages\n",
        "# 2. Train them on your full datasets (lang1_sentences and lang2_sentences)\n",
        "# 3. Inspect the models to see what n-grams they learned\n",
        "#\n",
        "# Example:\n",
        "# model_2gram_lang1 = NgramLanguageModel(n=2)\n",
        "# model_2gram_lang1.train(lang1_sentences)\n",
        "# model_3gram_lang1 = NgramLanguageModel(n=3)\n",
        "# model_3gram_lang1.train(lang1_sentences)\n",
        "#\n",
        "# model_2gram_lang2 = NgramLanguageModel(n=2)\n",
        "# model_2gram_lang2.train(lang2_sentences)\n",
        "# model_3gram_lang2 = NgramLanguageModel(n=3)\n",
        "# model_3gram_lang2.train(lang3_sentences)\n",
        "#\n",
        "# Display sample n-grams and their counts from each model\n",
        "# Check vocabulary size: len(model.model.vocab)\n",
        "# Show most common n-grams or test probabilities on sample sentences\n",
        "\n",
        "# 1. Create 2-gram and 3-gram models for both languages\n",
        "model_2gram_lang1 = CharNgramLanguageModel(n=2)\n",
        "model_3gram_lang1 = CharNgramLanguageModel(n=3)\n",
        "\n",
        "model_2gram_lang2 = CharNgramLanguageModel(n=2)\n",
        "model_3gram_lang2 = CharNgramLanguageModel(n=3)\n",
        "\n",
        "# 2. Train them on your full datasets (lang1_sentences and lang2_sentences)\n",
        "model_2gram_lang1.train(lang1_sentences)\n",
        "model_3gram_lang1.train(lang1_sentences)\n",
        "\n",
        "model_2gram_lang2.train(lang2_sentences)\n",
        "model_3gram_lang2.train(lang2_sentences)\n",
        "\n",
        "# 3. Inspect the models\n",
        "\n",
        "print(\"===== Vocabulary sizes =====\")\n",
        "print(\"Lang1 2-gram vocab size:\", len(model_2gram_lang1.model.vocab))\n",
        "print(\"Lang1 3-gram vocab size:\", len(model_3gram_lang1.model.vocab))\n",
        "print(\"Lang2 2-gram vocab size:\", len(model_2gram_lang2.model.vocab))\n",
        "print(\"Lang2 3-gram vocab size:\", len(model_3gram_lang2.model.vocab))\n",
        "\n",
        "\n",
        "# Helper function to print some example n-grams and counts\n",
        "def print_some_char_ngrams(model, n_order=2, limit=10):\n",
        "    \"\"\"\n",
        "    Print a few example character n-grams and their counts from a trained model.\n",
        "    \"\"\"\n",
        "    counter = model.model.counts[n_order]   # n-gram level (2 or 3)\n",
        "    printed = 0\n",
        "    for context in counter:\n",
        "        for ch, cnt in counter[context].items():\n",
        "            ngram = \"\".join(context) + ch\n",
        "            print(f\"{repr(ngram)} -> count={cnt}\")\n",
        "            printed += 1\n",
        "            if printed >= limit:\n",
        "                return\n",
        "\n",
        "print(\"\\n===== Sample 2-gram char n-grams (Lang1) =====\")\n",
        "print_some_char_ngrams(model_2gram_lang1, n_order=2, limit=10)\n",
        "\n",
        "print(\"\\n===== Sample 2-gram char n-grams (Lang2) =====\")\n",
        "print_some_char_ngrams(model_2gram_lang2, n_order=2, limit=10)\n",
        "\n",
        "# Test probabilities on a couple of sample sentences\n",
        "if len(lang1_sentences) > 0:\n",
        "    sample1 = lang1_sentences[0]\n",
        "    print(\"\\nExample Lang1 sentence:\", sample1[:80], \"...\")\n",
        "    print(\"P_2gram(Lang1) =\", model_2gram_lang1.get_probability(sample1))\n",
        "    print(\"P_3gram(Lang1) =\", model_3gram_lang1.get_probability(sample1))\n",
        "\n",
        "if len(lang2_sentences) > 0:\n",
        "    sample2 = lang2_sentences[0]\n",
        "    print(\"\\nExample Lang2 sentence:\", sample2[:80], \"...\")\n",
        "    print(\"P_2gram(Lang2) =\", model_2gram_lang2.get_probability(sample2))\n",
        "    print(\"P_3gram(Lang2) =\", model_3gram_lang2.get_probability(sample2))\n",
        "\n",
        "\n",
        "# [4 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olw1RKdrRHqG"
      },
      "source": [
        "## 2.2: Implement Language Identification (8 points)\n",
        "\n",
        "Create a function that compares sentence probabilities from two language models and returns the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ygVJnY5gRHqG"
      },
      "outputs": [],
      "source": [
        "def identify_language(sentence: str,\n",
        "                     model_lang1: CharNgramLanguageModel,\n",
        "                     model_lang2: CharNgramLanguageModel) -> int:\n",
        "    \"\"\"\n",
        "    Identify the language of a sentence using two character-based language models.\n",
        "\n",
        "    Args:\n",
        "        sentence: Sentence string\n",
        "        model_lang1: Language model for language 1 (label 0)\n",
        "        model_lang2: Language model for language 2 (label 1)\n",
        "\n",
        "    Returns:\n",
        "        Predicted label (0 or 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Probability from language 1 model\n",
        "    prob1 = model_lang1.get_probability(sentence)\n",
        "\n",
        "    # 2. Probability from language 2 model\n",
        "    prob2 = model_lang2.get_probability(sentence)\n",
        "\n",
        "    # 3. Compare and choose the higher probability\n",
        "    if prob1 > prob2:\n",
        "        return 0   # language 1\n",
        "    else:\n",
        "        return 1   # language 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v9cJXjQRHqG"
      },
      "source": [
        "## 2.3: Implement Evaluation Function (6 points)\n",
        "\n",
        "Create a function that calculates accuracy, precision, recall, and F1-score given predicted and true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QsO4Rwz6RHqH"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_metrics(y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with accuracy, precision, recall, f1_score\n",
        "    \"\"\"\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Precision, Recall, F1 (binary classification)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpUfSf2RHqH"
      },
      "source": [
        "## 2.4: 10-Fold Cross-Validation for Language Identification (8 points)\n",
        "\n",
        "Implement 10-fold cross-validation to evaluate your character-based n-gram models. In each fold, split the data, train separate models for each language and n-gram order, make predictions, and evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sqLvr7twRHqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fb697d-523d-41a3-9c70-dec9f1fb4961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset prepared:\n",
            "  Total sentences: 1944\n",
            "  Language 1 (label 0): 1629 sentences\n",
            "  Language 2 (label 1): 315 sentences\n",
            "\n",
            "\n",
            "==================================================\n",
            "Fold 1/10\n",
            "==================================================\n",
            "  Train size: 1749, Test size: 195\n",
            "  Train Lang1: 1465, Train Lang2: 284\n",
            "  2-gram -> acc: 0.990, prec: 0.939, rec: 1.000, f1: 0.969\n",
            "  3-gram -> acc: 0.995, prec: 0.969, rec: 1.000, f1: 0.984\n",
            "\n",
            "==================================================\n",
            "Fold 2/10\n",
            "==================================================\n",
            "  Train size: 1749, Test size: 195\n",
            "  Train Lang1: 1468, Train Lang2: 281\n",
            "  2-gram -> acc: 0.969, prec: 0.850, rec: 1.000, f1: 0.919\n",
            "  3-gram -> acc: 0.974, prec: 0.872, rec: 1.000, f1: 0.932\n",
            "\n",
            "==================================================\n",
            "Fold 3/10\n",
            "==================================================\n",
            "  Train size: 1749, Test size: 195\n",
            "  Train Lang1: 1464, Train Lang2: 285\n",
            "  2-gram -> acc: 0.933, prec: 0.698, rec: 1.000, f1: 0.822\n",
            "  3-gram -> acc: 0.969, prec: 0.833, rec: 1.000, f1: 0.909\n",
            "\n",
            "==================================================\n",
            "Fold 4/10\n",
            "==================================================\n",
            "  Train size: 1749, Test size: 195\n",
            "  Train Lang1: 1469, Train Lang2: 280\n",
            "  2-gram -> acc: 0.949, prec: 0.778, rec: 1.000, f1: 0.875\n",
            "  3-gram -> acc: 0.974, prec: 0.875, rec: 1.000, f1: 0.933\n",
            "\n",
            "==================================================\n",
            "Fold 5/10\n",
            "==================================================\n",
            "  Train size: 1750, Test size: 194\n",
            "  Train Lang1: 1466, Train Lang2: 284\n",
            "  2-gram -> acc: 0.979, prec: 0.886, rec: 1.000, f1: 0.939\n",
            "  3-gram -> acc: 0.985, prec: 0.912, rec: 1.000, f1: 0.954\n",
            "\n",
            "==================================================\n",
            "Fold 6/10\n",
            "==================================================\n",
            "  Train size: 1750, Test size: 194\n",
            "  Train Lang1: 1458, Train Lang2: 292\n",
            "  2-gram -> acc: 0.985, prec: 0.885, rec: 1.000, f1: 0.939\n",
            "  3-gram -> acc: 0.985, prec: 0.885, rec: 1.000, f1: 0.939\n",
            "\n",
            "==================================================\n",
            "Fold 7/10\n",
            "==================================================\n",
            "  Train size: 1750, Test size: 194\n",
            "  Train Lang1: 1461, Train Lang2: 289\n",
            "  2-gram -> acc: 0.979, prec: 0.867, rec: 1.000, f1: 0.929\n",
            "  3-gram -> acc: 0.985, prec: 0.897, rec: 1.000, f1: 0.945\n",
            "\n",
            "==================================================\n",
            "Fold 8/10\n",
            "==================================================\n",
            "  Train size: 1750, Test size: 194\n",
            "  Train Lang1: 1465, Train Lang2: 285\n",
            "  2-gram -> acc: 0.969, prec: 0.833, rec: 1.000, f1: 0.909\n",
            "  3-gram -> acc: 0.974, prec: 0.857, rec: 1.000, f1: 0.923\n",
            "\n",
            "==================================================\n",
            "Fold 9/10\n",
            "==================================================\n",
            "  Train size: 1750, Test size: 194\n",
            "  Train Lang1: 1485, Train Lang2: 265\n",
            "  2-gram -> acc: 0.979, prec: 0.926, rec: 1.000, f1: 0.962\n",
            "  3-gram -> acc: 0.990, prec: 0.962, rec: 1.000, f1: 0.980\n",
            "\n",
            "==================================================\n",
            "Fold 10/10\n",
            "==================================================\n",
            "  Train size: 1750, Test size: 194\n",
            "  Train Lang1: 1460, Train Lang2: 290\n",
            "  2-gram -> acc: 0.959, prec: 0.758, rec: 1.000, f1: 0.862\n",
            "  3-gram -> acc: 0.974, prec: 0.833, rec: 1.000, f1: 0.909\n",
            "\n",
            "==================================================\n",
            "Cross-validation completed!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Prepare dataset: combine sentence STRINGS from both languages with labels\n",
        "X = lang1_sentences + lang2_sentences\n",
        "y = [0] * len(lang1_sentences) + [1] * len(lang2_sentences)\n",
        "\n",
        "print(f\"Dataset prepared:\")\n",
        "print(f\"  Total sentences: {len(X)}\")\n",
        "print(f\"  Language 1 (label 0): {sum(1 for label in y if label == 0)} sentences\")\n",
        "print(f\"  Language 2 (label 1): {sum(1 for label in y if label == 1)} sentences\")\n",
        "print()\n",
        "\n",
        "# Initialize 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results for each fold\n",
        "results_2gram = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "results_3gram = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "\n",
        "# 10-fold cross-validation\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X), 1):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Fold {fold_idx}/10\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # 1. Split data into train and test sets\n",
        "    X_train = [X[i] for i in train_idx]\n",
        "    y_train = [y[i] for i in train_idx]\n",
        "    X_test  = [X[i] for i in test_idx]\n",
        "    y_test  = [y[i] for i in test_idx]\n",
        "\n",
        "    # 2. Separate training sentences by language based on labels\n",
        "    train_lang1_sentences = [sent for sent, label in zip(X_train, y_train) if label == 0]\n",
        "    train_lang2_sentences = [sent for sent, label in zip(X_train, y_train) if label == 1]\n",
        "\n",
        "    print(f\"  Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "    print(f\"  Train Lang1: {len(train_lang1_sentences)}, Train Lang2: {len(train_lang2_sentences)}\")\n",
        "\n",
        "    # 3. Train FOUR models total:\n",
        "    #    - Two 2-gram models (one per language)\n",
        "    #    - Two 3-gram models (one per language)\n",
        "\n",
        "    # 2-gram models\n",
        "    model2_lang1 = CharNgramLanguageModel(n=2)\n",
        "    model2_lang2 = CharNgramLanguageModel(n=2)\n",
        "    model2_lang1.train(train_lang1_sentences)\n",
        "    model2_lang2.train(train_lang2_sentences)\n",
        "\n",
        "    # 3-gram models\n",
        "    model3_lang1 = CharNgramLanguageModel(n=3)\n",
        "    model3_lang2 = CharNgramLanguageModel(n=3)\n",
        "    model3_lang1.train(train_lang1_sentences)\n",
        "    model3_lang2.train(train_lang2_sentences)\n",
        "\n",
        "    # 4. Make predictions on test sentences\n",
        "    y_pred_2gram = []\n",
        "    y_pred_3gram = []\n",
        "\n",
        "    for sent in X_test:\n",
        "        # 2-gram prediction\n",
        "        y_pred_2gram.append(\n",
        "            identify_language(sent, model2_lang1, model2_lang2)\n",
        "        )\n",
        "        # 3-gram prediction\n",
        "        y_pred_3gram.append(\n",
        "            identify_language(sent, model3_lang1, model3_lang2)\n",
        "        )\n",
        "\n",
        "    # 5. Calculate metrics\n",
        "    metrics_2 = calculate_metrics(y_test, y_pred_2gram)\n",
        "    metrics_3 = calculate_metrics(y_test, y_pred_3gram)\n",
        "\n",
        "    # 6. Store results\n",
        "    for k in results_2gram.keys():\n",
        "        results_2gram[k].append(metrics_2[k])\n",
        "        results_3gram[k].append(metrics_3[k])\n",
        "\n",
        "    # Fold sonuçlarını ekrana yaz\n",
        "    print(\"  2-gram -> acc: {:.3f}, prec: {:.3f}, rec: {:.3f}, f1: {:.3f}\".format(\n",
        "        metrics_2[\"accuracy\"], metrics_2[\"precision\"], metrics_2[\"recall\"], metrics_2[\"f1_score\"]\n",
        "    ))\n",
        "    print(\"  3-gram -> acc: {:.3f}, prec: {:.3f}, rec: {:.3f}, f1: {:.3f}\".format(\n",
        "        metrics_3[\"accuracy\"], metrics_3[\"precision\"], metrics_3[\"recall\"], metrics_3[\"f1_score\"]\n",
        "    ))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Cross-validation completed!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlXE_SfFRHqH"
      },
      "source": [
        "## 2.5: Display Results (12)\n",
        "\n",
        "*Create a table showing for each model:*\n",
        "Mean accuracy, precision, recall, F1 (with std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cyBVJGxFRHqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400a9b9b-23e8-4d7d-e626-621a6dec5639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Summary of 10-Fold Cross-Validation Results =====\n",
            "\n",
            " Model Accuracy (Mean ± Std) Precision (Mean ± Std) Recall (Mean ± Std) F1-Score (Mean ± Std)\n",
            "2-gram         0.969 ± 0.017          0.842 ± 0.073       1.000 ± 0.000         0.912 ± 0.044\n",
            "3-gram         0.980 ± 0.008          0.889 ± 0.045       1.000 ± 0.000         0.941 ± 0.025\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd # Assuming pandas is also needed for DataFrame\n",
        "\n",
        "# TODO: Calculate and display summary statistics\n",
        "#\n",
        "#\n",
        "# Example:\n",
        "# results_df = pd.DataFrame({\n",
        "#     'Model': ['2-gram', '3-gram'],\n",
        "#     'Accuracy': [...],\n",
        "#     'Precision': [...],\n",
        "#     ...\n",
        "# })\n",
        "\n",
        "# Calculate mean and standard deviation for 2-gram model\n",
        "mean_acc_2 = np.mean(results_2gram['accuracy'])\n",
        "std_acc_2 = np.std(results_2gram['accuracy'])\n",
        "mean_prec_2 = np.mean(results_2gram['precision'])\n",
        "std_prec_2 = np.std(results_2gram['precision'])\n",
        "mean_rec_2 = np.mean(results_2gram['recall'])\n",
        "std_rec_2 = np.std(results_2gram['recall'])\n",
        "mean_f1_2 = np.mean(results_2gram['f1_score'])\n",
        "std_f1_2 = np.std(results_2gram['f1_score'])\n",
        "\n",
        "# Calculate mean and standard deviation for 3-gram model\n",
        "mean_acc_3 = np.mean(results_3gram['accuracy'])\n",
        "std_acc_3 = np.std(results_3gram['accuracy'])\n",
        "mean_prec_3 = np.mean(results_3gram['precision'])\n",
        "std_prec_3 = np.std(results_3gram['precision'])\n",
        "mean_rec_3 = np.mean(results_3gram['recall'])\n",
        "std_rec_3 = np.std(results_3gram['recall'])\n",
        "mean_f1_3 = np.mean(results_3gram['f1_score'])\n",
        "std_f1_3 = np.std(results_3gram['f1_score'])\n",
        "\n",
        "# Create a DataFrame for better display\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['2-gram', '3-gram'],\n",
        "    'Accuracy (Mean ± Std)': [\n",
        "        f\"{mean_acc_2:.3f} ± {std_acc_2:.3f}\",\n",
        "        f\"{mean_acc_3:.3f} ± {std_acc_3:.3f}\"\n",
        "    ],\n",
        "    'Precision (Mean ± Std)': [\n",
        "        f\"{mean_prec_2:.3f} ± {std_prec_2:.3f}\",\n",
        "        f\"{mean_prec_3:.3f} ± {std_prec_3:.3f}\"\n",
        "    ],\n",
        "    'Recall (Mean ± Std)': [\n",
        "        f\"{mean_rec_2:.3f} ± {std_rec_2:.3f}\",\n",
        "        f\"{mean_rec_3:.3f} ± {std_rec_3:.3f}\"\n",
        "    ],\n",
        "    'F1-Score (Mean ± Std)': [\n",
        "        f\"{mean_f1_2:.3f} ± {std_f1_2:.3f}\",\n",
        "        f\"{mean_f1_3:.3f} ± {std_f1_3:.3f}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n===== Summary of 10-Fold Cross-Validation Results =====\\n\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# [4 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7XedIvGRHqH"
      },
      "source": [
        "**Question 2.1:** Which of your trained models performed best on the validation data, and why? (3-4 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz1s_WlARHqH"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckv543M4RHqH"
      },
      "source": [
        "**Question 2.2:** Were the results consistent across different folds of cross-validation? (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkbHF5HbRHqH"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk7uadeZRHqH"
      },
      "source": [
        "## 2.6: Out-of-Vocabulary Testing (12 pts)\n",
        "\n",
        "Test your models with **five** sentences containing characters or character combinations not common in your training corpus. For character n-grams, this might include unusual letter combinations, foreign words, or made-up words that still follow language patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dj0QEDnARHqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e161351-1d00-45a0-d3ab-ebb2119b36e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== OOV Sentence Testing =====\n",
            "\n",
            "OOV Sentence 1: Çelvünleşmiş töbrikşenme yapısı şörkütgünlü bir toprağa düştü.\n",
            "  2-gram -> predicted language: 1 (P1=1.62e-133, P2=4.69e-109)\n",
            "  3-gram -> predicted language: 1 (P1=4.47e-118, P2=1.49e-113)\n",
            "\n",
            "OOV Sentence 2: The zellartonic core emitted scryphtonic pulses across the quantisphere.\n",
            "  2-gram -> predicted language: 0 (P1=3.24e-89, P2=2.42e-134)\n",
            "  3-gram -> predicted language: 0 (P1=2.73e-95, P2=1.84e-131)\n",
            "\n",
            "OOV Sentence 3: Sträxønite reacted with løfthællium inside the mülgrøxen chamber.\n",
            "  2-gram -> predicted language: 0 (P1=3.77e-96, P2=6.99e-128)\n",
            "  3-gram -> predicted language: 0 (P1=1.06e-92, P2=3.82e-120)\n",
            "\n",
            "OOV Sentence 4: Bürsünteşmiş sümünle  gövüntei sismik modeli bozdu.\n",
            "  2-gram -> predicted language: 1 (P1=2.49e-98, P2=9.51e-82)\n",
            "  3-gram -> predicted language: 1 (P1=1.56e-97, P2=7.68e-86)\n",
            "\n",
            "OOV Sentence 5: A gravistorm event fractured the cryphexial driftline entirely.\n",
            "  2-gram -> predicted language: 0 (P1=4.80e-79, P2=5.90e-108)\n",
            "  3-gram -> predicted language: 0 (P1=3.50e-77, P2=7.86e-119)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2.6: Out-of-Vocabulary Testing\n",
        "\n",
        "# Önce tam korpus üzerinde yeni modeller eğitelim (tüm cümleler)\n",
        "full_2_lang1 = CharNgramLanguageModel(n=2)\n",
        "full_2_lang2 = CharNgramLanguageModel(n=2)\n",
        "full_3_lang1 = CharNgramLanguageModel(n=3)\n",
        "full_3_lang2 = CharNgramLanguageModel(n=3)\n",
        "\n",
        "full_2_lang1.train(lang1_sentences)\n",
        "full_2_lang2.train(lang2_sentences)\n",
        "full_3_lang1.train(lang1_sentences)\n",
        "full_3_lang2.train(lang2_sentences)\n",
        "\n",
        "# 5 tane OOV cümlesi (tuhaf karakterler, uydurma kelimeler, karışık diller)\n",
        "oov_sentences = [\n",
        "    \"Çelvünleşmiş töbrikşenme yapısı şörkütgünlü bir toprağa düştü.\",          # İngilizce yapı + uydurma kelime / harf\n",
        "    \"The zellartonic core emitted scryphtonic pulses across the quantisphere.\", # Türkçe yapı + yabancı/uydurma kelime\n",
        "    \"Sträxønite reacted with løfthællium inside the mülgrøxen chamber.\",# İngilizce + İskandinav harfleri\n",
        "    \"Bürsünteşmiş sümünle  gövüntei sismik modeli bozdu.\", # Türkçe + bozuk İngilizce / umlaut\n",
        "    \"A gravistorm event fractured the cryphexial driftline entirely.\", # İngilizce + bozuk karakterler\n",
        "]\n",
        "\n",
        "print(\"===== OOV Sentence Testing =====\\n\")\n",
        "\n",
        "for i, s in enumerate(oov_sentences, 1):\n",
        "    # 2-gram prediction\n",
        "    prob1_2 = full_2_lang1.get_probability(s)\n",
        "    prob2_2 = full_2_lang2.get_probability(s)\n",
        "    pred_2 = 0 if prob1_2 > prob2_2 else 1\n",
        "\n",
        "    # 3-gram prediction\n",
        "    prob1_3 = full_3_lang1.get_probability(s)\n",
        "    prob2_3 = full_3_lang2.get_probability(s)\n",
        "    pred_3 = 0 if prob1_3 > prob2_3 else 1\n",
        "\n",
        "    print(f\"OOV Sentence {i}: {s}\")\n",
        "    print(f\"  2-gram -> predicted language: {pred_2} (P1={prob1_2:.2e}, P2={prob2_2:.2e})\")\n",
        "    print(f\"  3-gram -> predicted language: {pred_3} (P1={prob1_3:.2e}, P2={prob2_3:.2e})\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# [8 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqrkgUOARHqH"
      },
      "source": [
        "**Question 2.3:** How well did your models handle out-of-vocabulary (OOV) samples? (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taFkBF3JRHqH"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz19fjohRHqH"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 3: Statistical Analysis (20 points)\n",
        "\n",
        "**Baseline (10 pts):** Statistical significance testing and comparison.  \n",
        "**Creativity (10 pts):** Advanced analysis (confusion matrices, error analysis, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peCDCXqqRHqO"
      },
      "source": [
        "## 3.1: Statistical Significance Testing (10 points)\n",
        "\n",
        "Use paired t-test to compare models. p-value < 0.05 indicates statistically significant difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nNrEt4s5RHqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3954df56-d28d-45de-c66f-911f48589589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Paired t-tests: 2-gram vs 3-gram =====\n",
            "\n",
            "ACCURACY:\n",
            "\n",
            "  t-statistic = 3.1673\n",
            "  p-value     = 0.0114\n",
            "  → Statistically significant difference (p < 0.05)\n",
            "\n",
            "PRECISION:\n",
            "\n",
            "  t-statistic = 3.5904\n",
            "  p-value     = 0.0058\n",
            "  → Statistically significant difference (p < 0.05)\n",
            "\n",
            "RECALL:\n",
            "\n",
            "  t-statistic = nan\n",
            "  p-value     = nan\n",
            "  → NOT statistically significant (p ≥ 0.05)\n",
            "\n",
            "F1_SCORE:\n",
            "\n",
            "  t-statistic = 3.3371\n",
            "  p-value     = 0.0087\n",
            "  → Statistically significant difference (p < 0.05)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import ttest_rel\n",
        "\n",
        "print(\"===== Paired t-tests: 2-gram vs 3-gram =====\\n\")\n",
        "\n",
        "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "    t_stat, p_value = ttest_rel(results_3gram[metric], results_2gram[metric])\n",
        "    print(f\"{metric.upper()}:\\n\")\n",
        "    print(f\"  t-statistic = {t_stat:.4f}\")\n",
        "    print(f\"  p-value     = {p_value:.4f}\")\n",
        "\n",
        "    if p_value < 0.05:\n",
        "        print(\"  → Statistically significant difference (p < 0.05)\\n\")\n",
        "    else:\n",
        "        print(\"  → NOT statistically significant (p ≥ 0.05)\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20soxRd6RHqO"
      },
      "source": [
        "**Question 3.1:** Are the performance differences statistically significant? Explain what 'statistical significance' means in this context. (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48KiLvz4RHqP"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lms2PaRRHqP"
      },
      "source": [
        "## 3.2: Advanced Analysis (10 points)\n",
        "\n",
        "Perform deeper analysis such as per-language performance, misclassification patterns, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvOfyE8ORHqP"
      },
      "outputs": [],
      "source": [
        "# TODO: Your advanced analysis here\n",
        "\n",
        "# [6 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQxhIhmMRHqP"
      },
      "source": [
        "**Question 3.2:** What interesting patterns or insights did you discover from your results? (4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tj2BkoiRHqP"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLhYIeKiG01-"
      },
      "source": [
        "# Convert Your Colab Notebook to PDF\n",
        "\n",
        "### Step 1: Download Your Notebook\n",
        "- Go to **File → Download → Download .ipynb**\n",
        "- Save the file to your computer\n",
        "\n",
        "### Step 2: Upload to Colab\n",
        "- Click the **📁 folder icon** on the left sidebar\n",
        "- Click the **upload button**\n",
        "- Select your downloaded .ipynb file\n",
        "\n",
        "### Step 3: Run the Code Below\n",
        "- **Uncomment the cell below** and run the cell\n",
        "- This will take about 1-2 minutes to install required packages\n",
        "- When prompted, type your notebook name (e.g.`gs_000000_as2.ipynb`) and press Enter\n",
        "\n",
        "### The PDF will be automatically downloaded to your computer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDPvhmqPMCSg"
      },
      "outputs": [],
      "source": [
        "# # Install required packages (this takes about 30 seconds)\n",
        "# print(\"Installing PDF converter... please wait...\")\n",
        "# !apt-get update -qq\n",
        "# !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc > /dev/null 2>&1\n",
        "# !pip install -q nbconvert\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# # Get notebook name from user\n",
        "# notebook_name = input(\"\\nEnter your notebook name: \")\n",
        "\n",
        "# # Add .ipynb if missing\n",
        "# if not notebook_name.endswith('.ipynb'):\n",
        "#     notebook_name += '.ipynb'\n",
        "\n",
        "# import os\n",
        "# notebook_path = f'/content/{notebook_name}'\n",
        "\n",
        "# # Check if file exists\n",
        "# if not os.path.exists(notebook_path):\n",
        "#     print(f\"\\n⚠ Error: '{notebook_name}' not found in /content/\")\n",
        "#     print(\"\\nMake sure you uploaded the file using the folder icon (📁) on the left!\")\n",
        "# else:\n",
        "#     print(f\"\\n✓ Found {notebook_name}\")\n",
        "#     print(\"Converting to PDF... this may take 1-2 minutes...\\n\")\n",
        "\n",
        "#     # Convert the notebook to PDF\n",
        "#     !jupyter nbconvert --to pdf \"{notebook_path}\"\n",
        "\n",
        "#     # Download the PDF\n",
        "#     from google.colab import files\n",
        "#     pdf_name = notebook_name.replace('.ipynb', '.pdf')\n",
        "#     pdf_path = f'/content/{pdf_name}'\n",
        "\n",
        "#     if os.path.exists(pdf_path):\n",
        "#         print(\"✓ SUCCESS! Downloading your PDF now...\")\n",
        "#         files.download(pdf_path)\n",
        "#         print(\"\\n✓ Done! Check your downloads folder.\")\n",
        "#     else:\n",
        "#         print(\"⚠ Error: Could not create PDF\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}